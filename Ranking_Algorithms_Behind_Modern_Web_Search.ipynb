{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "482bf6c0-6035-4968-a100-9748f78b27ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c9849f-3a83-4363-925b-8238abd3813f",
   "metadata": {},
   "source": [
    "#  _Ranking Algorithms Behind Modern Web Search_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61543cb1-abe3-4faa-a627-cf4090c5d89a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab763df-9cc5-4920-be60-131945d10dc8",
   "metadata": {},
   "source": [
    "**Abstract**\n",
    "\n",
    "The primary goal of this project is to explore how modern web search engines work by investigating the mathematical foundations and practical implementations of key algorithms, including **PageRank**, **HITS**, and **BM25**.\n",
    "The project begins with an overview of the evolution of **web search**, focusing on the role of **Google Search**, and implementation of algorithms. I explore some historical perspective which sets the stage for a deep theoretical and practical dive into the core mechanisms behind these algorithms.\n",
    "I explore the mathematical underpinnings of link-based ranking systems, including concepts such as **graph theory**, **Markov chains**, **stochastic matrices**, **eigenvectors**, **stationary distributions**, and the **random surfer model**. Each algorithm is implemented from scratch using **Python**, illustrating their logic, behavior, and impact on page ranking and information retrieval.\n",
    "To support **understanding**, the project includes:\n",
    "\n",
    "- Clear mathematical formulations\n",
    "\n",
    "- Code walkthroughs and Python implementations\n",
    "\n",
    "- Visualizations and Images\n",
    "\n",
    "- Comparative analysis of algorithm behavior\n",
    "\n",
    "\n",
    "This project aims to provide a comprehensive and comparative view of how modern search algorithms function work and why they remain foundational to web search technologies today. It serves a technical deep dive into information retrieval algorithms,testing algorithms and a tribute to the innovations that shaped the way we find information online.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9912c-e330-4370-8003-f5836a2db11c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0043854-39f6-4a36-ad6d-e2caf509b73e",
   "metadata": {},
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5bc1ab-d069-48e5-b7b1-c116f8161eee",
   "metadata": {},
   "source": [
    "## **What is Web Search?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009ba2f-09fa-473b-859d-cfe38e5b9f50",
   "metadata": {},
   "source": [
    "Web search refers to the process of finding information on the World Wide Web through **search engines**. When a user enters a query (typically a word or phrase), a search engine returns a list of relevant web pages ranked by relevance and importance.\n",
    "Definition of the web search we can say it is branch of **Information Retrieval (IR)** focused on **indexing**, **crawling**, **ranking**, and **retrieving web content** in response to user queries.\n",
    "\n",
    "\n",
    "### **Search Engine**\n",
    "A search engine is a software system that provides hyperlinks to web pages and other relevant information on the Web in response to a user's query.\n",
    "The user inputs a query within a web browser or a mobile app, and the search results are often a list of hyperlinks, accompanied by textual summaries and images. \n",
    "Users also have the option of limiting the search to a specific type of results, such as images, videos, or news.\n",
    "\n",
    "For a search provider, its engine is part of a distributed computing system that can encompass many data centers throughout the world. \n",
    "The speed and accuracy of an engine's response to a query is based on a complex system of **indexing** that is continuously updated by automated web crawlers.\\\n",
    "\n",
    "\n",
    "There have been many search engines since the dawn of the Web in the 1990s, but **Google Search** became the dominant one in the 2000s and has remained so.\n",
    "It currently has a **90% global market share**.\n",
    "\n",
    "\n",
    "## **How Web Search Works?** \n",
    "A search engine maintains the following processes in near real time:\n",
    "\n",
    "- **Crawling**\n",
    "- **Indexing**\n",
    "- **Processing**\n",
    "- **Ranking**\n",
    "- **Searching**\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://media.licdn.com/dms/image/D4D12AQG322OLRq4Ssg/article-cover_image-shrink_600_2000/0/1719918145487?e=2147483647&v=beta&t=ik5wi9nkxhus55h7ST1vlBEMQCI05Yjv4HluUHO_UJw\" alt=\"Image of How Search Engines Work\" width=\"300\" height=\"200\">\n",
    "</center>\n",
    "\n",
    "## Crawling\n",
    "Web search engines get their information by **web crawling** from site to site. The **spider** checks for the standard filename **robots.txt**, addressed to it. \n",
    "The **robots.txt** file contains directives for search spiders, telling it which pages to crawl and which pages not to crawl. \n",
    "After checking for **robots.txt** and either finding it or not, the spider sends certain information back to be indexed depending on many factors, such as the titles, page content, JavaScript, Cascading Style Sheets (CSS), headings, or its metadata in HTML meta tags. \n",
    "After a certain number of pages crawled, amount of data indexed, or time spent on the website, the spider stops crawling and moves on. \n",
    "No web crawler may actually crawl the entire reachable web. \n",
    "Due to infinite websites, spider traps, spam, and other exigencies of the real web, crawlers instead apply a crawl policy to determine when the crawling of a site should be deemed sufficient.\n",
    "Some websites are crawled exhaustively, while others are crawled only partially.\n",
    "\n",
    "## Indexing\n",
    "Indexing means associating words and other definable tokens found on web pages to their domain names and HTML-based fields.\n",
    "The associations are stored in a public database and accessible through web search queries. \n",
    "A query from a user can be a single word, multiple words or a sentence. \n",
    "The index helps find information relating to the query as quickly as possible.\n",
    "Some of the techniques for indexing, and caching are trade secrets, whereas web crawling is a straightforward process of visiting all sites on a systematic basis.\n",
    "\n",
    "\n",
    "## Searching\n",
    "Typically when a user enters a query into a search engine it is a few keywords.\n",
    "The index already has the names of the sites containing the keywords, and these are instantly obtained from the index. \n",
    "The real processing load is in generating the web pages that are the search results list.Every page in the entire list must be weighted according to information in the indexes.\n",
    "Then the top search result item requires the lookup, reconstruction, and markup of the snippets showing the context of the keywords matched. \n",
    "These are only part of the processing each search results web page requires, and further pages (next to the top) require more of this post-processing.\n",
    "Beyond simple keyword lookups, search engines offer their own **GUI**- or command-driven operators and search parameters to refine the search results. \n",
    "These provide the necessary controls for the user engaged in the feedback loop users create by filtering and weighting while refining the search results,\n",
    "given the initial pages of the first search results.\n",
    "\n",
    "\n",
    "Most search engines support the use of the Boolean operators AND, OR and NOT to help end users refine the search query. \n",
    "Boolean operators are for literal searches that allow the user to refine and extend the terms of the search. \n",
    "The engine looks for the words or phrases exactly as entered. \n",
    "Some search engines provide an advanced feature called **proximity search**, which allows users to define the distance between keywords.\n",
    "There is also **concept-based** searching where the research involves using statistical analysis on pages containing the words or phrases you search for.\n",
    "The usefulness of a search engine depends on the relevance of the result set it gives back.\n",
    "\n",
    "\n",
    "While there may be millions of web pages that include a particular word or phrase, some pages may be more relevant, popular, or authoritative than others. \n",
    "Most search engines employ methods to rank the results to provide best results first. \n",
    "How a search engine decides which pages are the best matches, and what order the results should be shown in, varies widely from one engine to another.\n",
    "The methods also change over time as Internet usage changes and new techniques evolve. \n",
    "There are two main types of search engine that have evolved: one is a **system of predefined and hierarchically ordered keywords** that humans have programmed extensively.\n",
    "The other is a system that generates an **inverted index** by analyzing texts it locates. \n",
    "This first form relies much more heavily on the computer itself to do the bulk of the work.\n",
    "\n",
    "\n",
    "Most Web search engines are commercial ventures supported by advertising revenue and thus some of them allow advertisers to have their listings ranked higher in search results for a fee. \n",
    "Search engines that do not accept money for their search results make money by running search related ads alongside the regular search engine results. \n",
    "The search engines make money every time someone clicks on one of these ads.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5ae7bc-766b-4aa5-ab7e-a72a1054b908",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904bb636-1955-45b3-8304-6484c4d536fc",
   "metadata": {},
   "source": [
    "## **Implementing Basic Crawling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "022464cf-b4c0-4bc3-9a82-1688075277d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url):\n",
    "    response = requests.get(url)\n",
    "    hrefs = []\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            hrefs.append(href)\n",
    "    else:\n",
    "        print(f\"Error crawling {url}: {e}\")   \n",
    "        \n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96126294-45e0-4b9e-952a-188d412b6560",
   "metadata": {},
   "outputs": [],
   "source": [
    "books_result =  crawl('https://books.toscrape.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "487f87ad-6b6a-46e5-bbdf-ba661408ad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "quotes_result = crawl('https://quotes.toscrape.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c7ec9f-04bb-4e0b-953d-f0b69136ec3d",
   "metadata": {},
   "source": [
    "## **Implementing Crawling and Indexing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "309dbc87-6214-4690-afd4-443301e3fdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages with 'book': {'https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html'}\n"
     ]
    }
   ],
   "source": [
    "# Inverted index: word → list of URLs\n",
    "inverted_index = defaultdict(set)\n",
    "\n",
    "\"\"\"tokenize function splits text to lower case and return list of matches of the regex\"\"\"\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\\\n",
    "\n",
    "\n",
    "\"\"\"crawl_and_index function make fetch request and if it succesfull:\n",
    "1.parse html,\n",
    "2.extract visible text,\n",
    "3.tokenize words\n",
    "4.Adds the current url to the index entry for each word.\n",
    "\"\"\"\n",
    "\n",
    "def crawl_and_index(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            text = soup.get_text()\n",
    "            words = tokenize(text)\n",
    "            for word in words:\n",
    "                inverted_index[word].add(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error crawling {url}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example: Crawling two book pages\n",
    "urls = [\n",
    "    \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\",\n",
    "    \"https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html\"\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    crawl_and_index(url)\n",
    "\n",
    "# Example: Find all pages containing the word \"book\"\n",
    "print(\"Pages with 'book':\", inverted_index[\"book\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17537495-2467-4af7-b871-ad38443404fa",
   "metadata": {},
   "source": [
    "## **Searching Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bd170f5-cb8e-4a9b-bc4a-66b3beef934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    query_words = tokenize(query)\n",
    "    if not query_words:\n",
    "        return set()\n",
    "\n",
    "    # Start with the set of URLs for the first word\n",
    "    results = inverted_index.get(query_words[0], set()).copy()\n",
    "\n",
    "    # Intersect with the URL sets for the rest of the words (AND search)\n",
    "    for word in query_words[1:]:\n",
    "        results &= inverted_index.get(word, set())\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c6841-427d-4761-a53c-8688516763cb",
   "metadata": {},
   "source": [
    "Combine all together and make **simple search engine**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3c085d-43e6-4117-9576-1446616dba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# Inverted index: maps each word → set of URLs that contain it\n",
    "inverted_index = defaultdict(set)\n",
    "\n",
    "# tokenize function: splits text into lowercase words using regex\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# crawl_and_index function:\n",
    "# 1. fetches HTML from a URL\n",
    "# 2. extracts visible text\n",
    "# 3. tokenizes the text into words\n",
    "# 4. adds the URL to the index for each word\n",
    "def crawl_and_index(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            text = soup.get_text()\n",
    "            words = tokenize(text)\n",
    "            for word in words:\n",
    "                inverted_index[word].add(url)\n",
    "    except Exception as e:\n",
    "        print(f\"Error crawling {url}: {e}\")\n",
    "\n",
    "# Search function: returns URLs containing ALL query words\n",
    "def search(query):\n",
    "    query_words = tokenize(query)\n",
    "    if not query_words:\n",
    "        return set()\n",
    "\n",
    "    # Start with the set of URLs for the first word\n",
    "    results = inverted_index.get(query_words[0], set()).copy()\n",
    "\n",
    "    # Intersect with the URL sets for the rest of the words (AND search)\n",
    "    for word in query_words[1:]:\n",
    "        results &= inverted_index.get(word, set())\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example: Crawl some book pages\n",
    "urls = [\n",
    "    \"https://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\",\n",
    "    \"https://books.toscrape.com/catalogue/tipping-the-velvet_999/index.html\"\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    crawl_and_index(url)\n",
    "\n",
    "# Command-line search loop\n",
    "while True:\n",
    "    query = input(\"\\nEnter search query (or 'exit'): \").strip()\n",
    "    if query.lower() == 'exit':\n",
    "        break\n",
    "    result_urls = search(query)\n",
    "    if result_urls:\n",
    "        print(\"Search results:\")\n",
    "        for url in result_urls:\n",
    "            print(\"-\", url)\n",
    "    else:\n",
    "        print(\"No results found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc36fbd-de09-46d1-9f39-ca2b75825ec8",
   "metadata": {},
   "source": [
    "## **History**\n",
    "### **Before 1990**\n",
    "In **1945**, **Vannevar Bush** described an information retrieval system that would allow a user to access a great expanse of information, all at a single desk.\n",
    "He called it a **memex**.The memex was intended to give a user the capability to overcome the ever-increasing difficulty of locating information in ever-growing centralized indices of scientific work. \n",
    "Vannevar Bush envisioned libraries of research with connected annotations, which are similar to modern hyperlinks.\n",
    "Link analysis nowadays are crucial component of search engines through algorithms such as **PageRank**.\n",
    "\n",
    "### **1990 Birth of search engines**\n",
    "The first well documented search engine that searched content files  was **Archie**, which debuted on **10 September 1990.**\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://hackaday.com/wp-content/uploads/2024/05/Archie_search_engine_thumb.jpg?w=600&h=600\" alt=\"Image of Archie search engine\" width=\"300\" height=\"200\">\n",
    "</center>\n",
    "\n",
    "\n",
    "It was created by Alan Emtage, computer science student at McGill University in Montreal, Quebec, Canada. \n",
    "The program downloaded the directory listings of all the files located on public anonymous FTP (File Transfer Protocol) sites, creating a searchable database of file names; \n",
    "However, Archie Search Engine did not index the contents of these sites since the amount of data was so limited it could be readily searched manually.\n",
    "\n",
    "\n",
    "After **Archie** was **Veronica** and **Jughead**!\n",
    "\n",
    "#### Veronica and Jughead\n",
    "\n",
    "Like **Archie**, they searched the file names and titles stored in **Gopher**(is s a communication protocol designed for distributing, searching, and retrieving documents in Internet Protocol networks) index systems.\n",
    "**Veronica** provided a keyword search of most Gopher menu titles in the entire Gopher listings. \n",
    "**Jughead**  was a tool for obtaining menu information from specific Gopher servers. \n",
    "\n",
    "#### Yahoo\n",
    "\n",
    "The first popular search engine on the Web was **Yahoo!** The first product from Yahoo!, founded by Jerry Yang and David Filo in January 1994, \n",
    "was a Web directory called **Yahoo! Directory**. In 1995, a search function was added, allowing users to search Yahoo! Directory.\n",
    "It became one of the most popular ways for people to find web pages of interest, but its search function operated on its web directory, \n",
    "rather than its full-text copies of web pages.\n",
    "\n",
    "After some years of creation search engines ,**PageRank** was born!\n",
    "\n",
    "### **History of PageRank**\n",
    "A search engine called **RankDex** from IDD Information Services, designed by **Robin Li** in 1996, developed a strategy for site-scoring and page-ranking.\n",
    "It was one of the first search engines to use link analysis-which is ranking the popularity of a web site based on how many other sites had linked to it for ranking pages meaning it also considered hyperlink structure, not just keywords.\n",
    "It is called **RankDex** and it was launched in 1996.**Li** filed a patent for the technology in **RankDex** in 1997.\n",
    "He later used it when he founded **Baidu** in China in 2000.\n",
    "Google founder **Larry Page** referenced Li's work as a citation in some of his U.S. patents for **PageRank**.\n",
    "\n",
    "\n",
    "**Larry Page** and **Sergey Brin** developed **PageRank** at Stanford University in 1996 as part of a research project about a new kind of search engine. Sergey Brin had the idea that information on the web could be ordered in a hierarchy by link popularity-a page ranks higher as there are more links to it.The system was developed with the help of Scott Hassan and Alan Steremberg, both of whom were cited by Page and Brin as being critical to the development of Google. Rajeev Motwani and Terry Winograd co-authored with Page and Brin the first paper about the project, describing **PageRank** and the initial prototype of the [Google search engine](https://en.wikipedia.org/wiki/Google_Search), published in 1998.\n",
    "> 📜 [*The Anatomy of a Large-Scale Hypertextual Web Search Engine*](http://infolab.stanford.edu/pub/papers/google.pdf)\n",
    "\n",
    "\n",
    "While just nowadays has many factors that determine the ranking of Google search results, **PageRank** continues to provide the basis for all of Google's web-search tools.The name **PageRank** plays on the name of developer **Larry Page**, as well as of the concept of a web page.The word is a trademark of Google, and the **PageRank** process has been patented assigned to Stanford University and not to Google.Google has exclusive license rights on the patent from Stanford University. The university received 1.8 million shares of Google in exchange for use of the patent.It sold the shares in 2005 for $336 million.\n",
    "\n",
    "## **The Problem of Web Search Before PageRank?**\n",
    "\n",
    "Before Google and the introduction of PageRank, web search engines struggled to deliver high-quality, relevant results. \n",
    "As the internet rapidly expanded during the 1990s, the number of web pages grew into the millions, then billions — but search technology failed to keep up with that growth in a meaningful way.\n",
    "Most early search engines  relied heavily on simple keyword matching to retrieve results. \n",
    "They would look for pages that contained the same words as the user's query, and rank them based on factors like:\n",
    "frequency of the keyword,location of the keyword,basic metadata.While this approach was easy to implement, it had major weaknesses:it was easy to manipulate ,it treated all pages as equally trustworthy or important and it didn’t consider how humans value content — through references and links.\n",
    "There was no good way to determine which pages were actually important or trustworthy. \n",
    "\n",
    "\n",
    "A personal blog and a university website could rank equally if they both mentioned the same keywords.\n",
    "This meant that relevance and quality often suffered.Search engines needed a way to filter spammy or low-quality pages,highlight content that was trusted or referenced by others,deliver more objective and useful results for users.\n",
    "This set the stage for a fundamentally new idea: using link structure to infer page importance — the idea behind **PageRank**.\n",
    "\n",
    "## 📜 Timeline: Evolution of Web Search and the Birth of PageRank\n",
    "\n",
    "| Year | Milestone |\n",
    "|------|-----------|\n",
    "| **1945** | **memex** |\n",
    "| **1990** | **Archie** | \n",
    "| **1990-1993** | **Veronica and Jughead** | \n",
    "| **1995** | **Yahoo** | \n",
    "| **1996** | **RankDex (Robin Li)** | \n",
    "| **1998** | **PageRank (Page & Brin)** |\n",
    "| **1998** | **Google Founded** | \n",
    "\n",
    "---\n",
    "\n",
    "Nowadays active search engine crawlers include those of **Google**, **Sogou**, **Baidu**, **Bing**, **Gigablast**, **Mojeek**, **DuckDuckGo** and **Yandex**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25bb90a-0e13-4400-bb9a-569417cbdfa9",
   "metadata": {},
   "source": [
    "So we already see **What is web search?**.Check **history** and **reason** to have search engines. But one question jump to us!\n",
    "\n",
    "## **Why Web Search is a Complex Problem?**\n",
    "While modern web search engines like **Google** seem simple to the user—just type and get answers—they are, in reality, built on one of the most complex engineering and mathematical systems ever developed. Several key challenges make web search incredibly difficult!The internet contains billions of pages and continues to grow every day. \n",
    "Search engines must:crawl and store huge amounts of content,\n",
    "keep the index updated in real-time,\n",
    "deliver results in milliseconds.Also users usually enter 2–3 word queries. These are often:\n",
    "ambiguous,\n",
    "misspelled or vague,\n",
    "missing key context.Understanding intent from such input requires sophisticated models, often using machine learning and natural language processing.At these days every search has relevence.\n",
    "What is relevant to one person might not be to another.\n",
    "Also Web contains both valuable information and misleading content. Search engines must:\n",
    "detect spam and low-quality pages,\n",
    "rank trustworthy, authoritative content higher,\n",
    "prevent manipulation. And many other complex things!\n",
    "The problem of web search arose from the need to navigate and extract meaning from the overwhelming volume of online content. \n",
    "The complexity of this task grows with the internet itself and demands continuous innovation in areas like graph theory, information retrieval, AI, and large-scale systems. \n",
    "Solving this problem has not only changed how we access information—but transformed society itself. \n",
    "Understanding the algorithms behind it gives us insight into one of the most powerful and influential technologies of the modern age."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a8941-681c-4d77-b5a8-e4496a8fb198",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f955335b-5b6a-4d5c-b9e4-0685d07e9940",
   "metadata": {},
   "source": [
    "## **The Anatomy of a Large-Scale Hypertextual Web Search Engine (1998)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2df10d6-8634-4484-bd81-17ecbeeedfac",
   "metadata": {},
   "source": [
    "This is all important info from _The Anatomy of a Large-Scale Hypertextual Web Search Engine (1998)_ and show how **Google** works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c022ca4a-1eb7-4f0f-879c-007d6f81d565",
   "metadata": {},
   "source": [
    "As I mentioned a little while ago in 1998, Larry Page and Sergey Brin published a paper titled **The Anatomy of a Large-Scale Hypertextual Web Search Engine.** This work introduced **Google** to the world and laid the foundation for modern web search engines.\n",
    "At the heart of their prototype was a novel ranking algorithm called **PageRank**, which changed the way search results were ordered on the web.\n",
    "They present **Google**, a prototype of a large-scale search engine which makes heavyuse of the structure present in hypertext designed to **crawl** and **index* the Web efficiently and produce much more satisfying search results than existing systems.\n",
    "\n",
    "\n",
    "Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms.\n",
    "They answer tens of millions of queries every day. \n",
    "Despite the importance of large-scale search engines on the web,very little academic research has been done on them.\n",
    "Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago.\n",
    "Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results.\n",
    "They addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. \n",
    "Also look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.\n",
    "\n",
    "\n",
    "The web creates new challenges for information retrieval. The amount of information on the web is\n",
    "growing rapidly, as well as the number of new users inexperienced in the art of web research. People are\n",
    "likely to surf the web using its **link graph**, often starting with high quality human maintained indices. \n",
    "Human maintained lists cover popular topics effectively but are\n",
    "subjective, expensive to build and maintain, slow to improve, and cannot cover all esoteric topics.\n",
    "\n",
    "\n",
    "Automated search engines that rely on keyword matching usually return too many low quality matches.\n",
    "To make matters worse, some advertisers attempt to gain people’s attention by taking measures meant to\n",
    "mislead automated search engines. They have built a large-scale search engine which addresses many of\n",
    "the problems of existing systems. It makes especially heavy use of the additional structure present in\n",
    "hypertext to provide much higher quality search results. \n",
    "\n",
    "\n",
    "They said they chose their system name, **Google**, because it\n",
    "is a common spelling of googol, or $10^{100}$ and fits well with they goal of building very large-scale search\n",
    "engines. \n",
    "\n",
    "Search engine technology has had to scale dramatically to keep up with the growth of the web. In 1994,\n",
    "one of the **first web search engines**, the **World Wide Web Worm (WWWW)** had an index\n",
    "of **110,000** web pages and web accessible documents. As of November, 1997, the top search engines\n",
    "claim to index from **2 million (WebCrawler) to 100 million web documents (from Search Engine\n",
    "Watch)**.The number of queries search engines handle has grown incredibly\n",
    "too. In March and April 1994, the World Wide Web Worm received an average of about **1500 queries\n",
    "per day**. In November 1997, reach **20 million** queries per day. \n",
    "\n",
    "\n",
    "With the increasing number of users on the web, and automated systems which query search engines, it is likely\n",
    "that top search engines will handle hundreds of millions of queries per day. The goal of\n",
    "they system is to address many of the problems, both in **quality** and **scalability**, introduced by scaling\n",
    "search engine technology to such extraordinary numbers.\n",
    "\n",
    "\n",
    "Creating a search engine which scales even to today’s web presents many challenges. \n",
    "- **Fast crawling**technology is needed to gather the web documents and keep them up to date.\n",
    "- **Storage space** must be used efficiently to store indices and, optionally, the documents themselves.\n",
    "- The **indexing system** must process hundreds of gigabytes of data efficiently.\n",
    "- **Queries** must be handled quickly, at a rate of hundreds to thousands per second.\n",
    "\n",
    "\n",
    "These tasks are becoming increasingly difficult as the Web grows. In designing Google,\n",
    "they have considered both the rate of growth of the Web and technological changes. They said Google is designed to\n",
    "scale well to extremely **large data sets**. It makes efficient use of storage space to **store the index**. Its data\n",
    "structures are optimized for fast and efficient access.\n",
    "\n",
    "\n",
    "They main goal is to improve the **quality** of web search engines. Some people believed that a\n",
    "complete search index would make it possible to find anything easily. According to Best of the Web\n",
    "1994 they said: The best navigation service should make it easy to find almost anything on the\n",
    "web once all the data is entered!\n",
    "\n",
    "\n",
    "Anyone who has used a search engine recently, can readily testify that the completeness of the index is not the only factor in\n",
    "the quality of search results. **Junk results** often wash out any results that a user is interested in.\n",
    "\n",
    "\n",
    "In fact,November 1997, one of the top four commercial search engines finds itself returns its own\n",
    "search page in response to its name in the top ten results. One of the main causes of this problem is that\n",
    "the number of documents in the indices has been increasing by many orders of magnitude, but the user’s\n",
    "ability to look at documents has not. People are still only willing to look at the first few tens of results.\n",
    "Because of this, as the collection size grows, they need tools that have very **high precision**.Indeed, they want their notion of **relevant** to\n",
    "only include the very best documents since there may be tens of thousands of slightly relevant\n",
    "documents. This very high precision is important even at the expense of recall (the total number of\n",
    "relevant documents the system is able to return). \n",
    "\n",
    "\n",
    "There is quite a bit of recent optimism that the use of\n",
    "more hypertextual information can help improve search and other applications.In particular, link structure  and link text provide a lot of\n",
    "information for making relevance judgments and quality filtering. Google makes use of both link\n",
    "structure and anchor text.\n",
    "\n",
    "\n",
    "Aside from tremendous growth, the Web has also become increasingly commercial over time. In 1993,\n",
    "1.5% of web servers were on .com domains. This number grew to over 60% in 1997. At the same time,\n",
    "search engines have migrated from the academic domain to the commercial. Up until now most search\n",
    "engine development has gone on at companies with little publication of technical details. This causes\n",
    "search engine technology to remain largely a black art and to be advertising oriented.\n",
    "They important design goal was to build systems that reasonable numbers of people can actually use.\n",
    "Usage was important to them because they think some of the most interesting research will involve\n",
    "leveraging the vast amount of usage data that is available from modern web systems.It is very difficult to get data,\n",
    "mainly because it is considered commercially valuable.\n",
    "Final design goal was to build an architecture that can support novel research activities on\n",
    "large-scale web data. To support novel research uses, Google stores all of the actual documents it crawls\n",
    "in **compressed form**. They designing Google to set up an environment where\n",
    "other researchers can come in quickly, process large chunks of the web, and produce interesting results\n",
    "that would have been very difficult to produce otherwise. In the short time the system has been up, there\n",
    "have already been several papers using databases generated by Google, and many others are underway.\n",
    "\n",
    "\n",
    "The Google search engine has two important features that help it produce high precision results. First, it\n",
    "makes use of the **link structure** of the Web to calculate a quality ranking for each web page. This ranking\n",
    "is called **PageRank**. Second, Google utilizes link to improve\n",
    "search results. \n",
    "\n",
    "They created maps containing **518 million** of these hyperlinks, a\n",
    "**significant sample** of the total. These maps allow rapid calculation of a web page’s **PageRank**(an\n",
    "objective measure of its citation importance that corresponds well with people’s subjective idea of\n",
    "importance). Because of this correspondence, **PageRank** is an excellent way to **prioritize the results** of\n",
    "web keyword searches. For most popular subjects, a simple text matching search that is restricted to web\n",
    "page titles performs admirably when PageRank prioritizes the results.\n",
    "\n",
    "#### **PageRank**\n",
    "\n",
    "Academic citation literature has been applied to the web, largely by counting citations or backlinks to a\n",
    "given page. This gives some approximation of a page’s importance or quality. PageRank extends this\n",
    "idea by not counting links from all pages equally, and by normalizing by the number of links on a page.\n",
    "\n",
    "*We assume page A has pages T1...Tn which point to it. The parameter d\n",
    "is a damping factor which can be set between 0 and 1. We usually set d to **0.85**. Also C(A) is defined as the number of links going\n",
    "out of page A. The PageRank of a page A is given as follows:*\n",
    "\n",
    "$$PR(A) = (1-d) + d (PR(T1)/C(T1) + ... + PR(Tn)/C(Tn))$$\n",
    "\n",
    "**PageRanks** form a **probability distribution** over web pages, so the sum of all web pages’ PageRanks will be one.\n",
    "**PageRank** or PR(A) can be calculated using a simple iterative algorithm, and corresponds to the\n",
    "principal **eigenvector** of the normalized **link matrix** of the web. \n",
    "\n",
    "\n",
    "PageRank can be thought of as a model of user behavior. We assume there is a **random surfer** who is\n",
    "given a web page at random and keeps clicking on links, **never hitting back** but eventually gets bored\n",
    "and starts on another random page. The probability that the random surfer visits a page is its **PageRank**.\n",
    "\n",
    "\n",
    "**Damping factor** is the probability at each page the **random surfer** will get bored and request\n",
    "another random page. \n",
    "\n",
    "\n",
    "Intuitive justification is that a page can have a high **PageRank** if there are many pages that point\n",
    "to it, or if there are some pages that point to it and have a high **PageRank**. Intuitively, pages that are well\n",
    "cited from many places around the web are worth looking at.\n",
    "\n",
    "\n",
    "#### **Link Texts**\n",
    "The text of links is treated in a special way in search engine. Most search engines associate the text\n",
    "of a link with the page that the link is on. They associate it with the page the link points to.\n",
    "This has several advantages. First, anchors often provide more accurate descriptions of web pages than\n",
    "the pages themselves. Second, anchors may exist for documents which cannot be indexed by a\n",
    "text-based search engine, such as images, programs, and databases. This makes it possible to return web\n",
    "pages which have not actually been crawled. Note that pages that have not been crawled can cause\n",
    "problems, since they are never checked for validity before being returned to the user. In this case, the\n",
    "search engine can even return a page that never actually existed, but had hyperlinks pointing to it.\n",
    "However, it is possible to sort the results, so that this particular problem rarely happens.\n",
    "This idea of propagating anchor text to the page it refers to was implemented in the World Wide Web\n",
    "Worm especially because it helps search non-text information, and expands the search\n",
    "coverage with fewer downloaded documents. We use anchor propagation mostly because anchor text\n",
    "can help provide better quality results. Using anchor text efficiently is technically difficult because of\n",
    "the large amounts of data which must be processed. In our current crawl of 24 million pages, we had\n",
    "over 259 million anchors which we indexed. \n",
    "\n",
    "\n",
    "### **Google Architecture Overview**\n",
    "In this section, they give a high level overview of how\n",
    "the whole system works.\n",
    "\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://www.researchgate.net/profile/Nureni-Azeez/publication/255644944/figure/download/fig1/AS:285567700615168@1445096053801/High-Level-Google-Architecture.png\" alt=\"Image of High Level Google Arhitecture\" width=\"300\" height=\"200\">\n",
    "</center>\n",
    "\n",
    "\n",
    "Most of **Google** is\n",
    "implemented in C or C++ for efficiency and can run in\n",
    "either Solaris or Linux.\n",
    "In Google, the **web crawling** (downloading of web pages)\n",
    "is done by several distributed crawlers. There is a\n",
    "**URLserver** that sends lists of **URLs** to be fetched to the\n",
    "crawlers. The web pages that are fetched are then sent to\n",
    "the **storeserver**. The storeserver then **compresses** and **stores**\n",
    "the web pages into a **repository**. Every web page has an\n",
    "associated ID number called a **docID** which is assigned\n",
    "whenever a new URL is parsed out of a web page. The\n",
    "**indexing function** is performed by the **indexer** and the\n",
    "**sorter**. The **indexer** performs a number of functions. It reads\n",
    "the repository, uncompresses the documents, and parses them. Each document is converted into a set of\n",
    "word occurrences called **hits**. The **hits** record the word, position in document, an approximation of font\n",
    "size, and capitalization. The **indexer** distributes these hits into a set of **barrels**, creating a partially\n",
    "sorted forward index. The indexer performs another important function. It parses out all the links in\n",
    "every web page and stores important information about them in an **anchors file**. This file contains\n",
    "enough information to determine where each link points from and to, and the text of the link.\n",
    "The **URLresolver** reads the anchors file and converts relative URLs into absolute URLs and in turn into\n",
    "**docIDs**. It puts the anchor text into the forward index, associated with the **docID** that the anchor points\n",
    "to. It also generates a database of links which are pairs of **docIDs**. The **links database** is used to compute\n",
    "**PageRanks** for all the documents.\n",
    "\n",
    "\n",
    "The **sorter** takes the **barrels**, which are sorted by **docID** and\n",
    "resorts them by **wordID** to generate the **inverted index**. This is done in place so that little temporary\n",
    "space is needed for this operation. The sorter also produces a list of **wordIDs** and **offsets** into the\n",
    "**inverted index**. A program called **DumpLexicon** takes this list together with the lexicon produced by the\n",
    "indexer and generates a new lexicon to be used by the searcher. The **searcher** is run by a web server and\n",
    "uses the lexicon built by **DumpLexicon** together with the **inverted index** and the **PageRanks** to answer\n",
    "queries. \n",
    "\n",
    "\n",
    "**Google’s data structures** are optimized so that a large document collection can be **crawled**, **indexed**, and\n",
    "**searched** with little cost.\n",
    "\n",
    "\n",
    "#### **Repositary**\n",
    "\n",
    "The **repository** contains the full HTML of every web page.\n",
    "Each page is compressed using **zlib**. The\n",
    "choice of compression technique is a tradeoff between speed\n",
    "and compression ratio. They chose zlib’s speed over a\n",
    "significant improvement in compression offered by bzip.In the\n",
    "repository, the documents are stored one after the other and\n",
    "are prefixed by **docID**, **length**, and **URL**.\n",
    "The repository requires no other data structures to be used in order to access it. This helps with\n",
    "data consistency and makes development much easier;\n",
    "\n",
    "**we can rebuild all the other data structures from\n",
    "only the repository and a file which lists crawler errors**. \n",
    "\n",
    "#### **Document Index**\n",
    "The document index keeps information about each document. It is a fixed width **ISAM** (Index sequential\n",
    "access mode) index, ordered by **docID**. The information stored in each entry includes the current\n",
    "document status, a pointer into the repository, a document checksum, and various statistics. If the\n",
    "document has been crawled, it also contains a pointer into a variable width file called **docinfo** which\n",
    "contains its URL and title. Otherwise the pointer points into the **URLlist** which contains just the URL.\n",
    "This design decision was driven by the desire to have a reasonably compact data structure, and the\n",
    "ability to fetch a record in one disk seek during a search.\n",
    "Additionally, there is a file which is used to convert **URLs** into **docID**s. It is a list of URL checksums\n",
    "with their corresponding docIDs and is sorted by checksum. In order to find the docID of a particular\n",
    "URL, the URL’s checksum is computed and a binary search is performed on the checksums file to find\n",
    "its docID. URLs may be converted into docIDs in batch by doing a merge with this file. This is the\n",
    "technique the URLresolver uses to turn URLs into docIDs. This batch mode of update is crucial because\n",
    "otherwise we must perform one seek for every link which assuming one disk would take more than a\n",
    "month for our **322 million** link dataset. \n",
    "\n",
    "\n",
    "#### **Hit List**\n",
    "A hit list corresponds to a list of occurrences of a particular word in a particular document including\n",
    "position, font, and capitalization information. Hit lists account for most of the space used in both the\n",
    "forward and the inverted indices. Because of this, it is important to represent them as efficiently as\n",
    "possible. Тhey considered several alternatives for encoding position, font, and capitalization -- simple\n",
    "encoding (a triple of integers), a compact encoding (a hand optimized allocation of bits), and **Huffman\n",
    "coding**. In the end we chose a **hand optimized compact encoding ** since it required far less space than the\n",
    "simple encoding and far less bit manipulation than **Huffman coding**.\n",
    "\n",
    "\n",
    "They compact encoding uses two bytes for every hit. There are two types of hits: **fancy hits** and **plain hits**.\n",
    "**Fancy hits** include hits occurring in a **URL**, **title**, **anchor text**, or **meta tag**. **Plain hits** include everything\n",
    "else. A plain hit consists of a **capitalization bit**, **font size**, and **12 bits** of word position in a document. \n",
    "**Font size** is represented relative to the rest of the document\n",
    "using **three bits** . A fancy hit consists of a capitalization bit, the **font size set to 7** to indicate it is a fancy hit, **4 bits to encode the\n",
    "type of fancy hit**, and **8 bits of position**. For **anchor hits**, the 8 bits of position are split into 4 bits for\n",
    "position in anchor and 4 bits for a hash of the docID the anchor occurs in. This gives us some limited\n",
    "phrase searching as long as there are not that many anchors for a particular word. They expect to update\n",
    "the way that anchor hits are stored to allow for greater resolution in the position and docIDhash fields.\n",
    "\n",
    "\n",
    "They use font size relative to the rest of the document because when searching, you do not want to rank\n",
    "otherwise identical documents differently just because one of the documents is in a larger font.\n",
    "\n",
    "The length of a hit list is stored before the hits themselves.\n",
    "To save space, the length of the hit list is combined with the\n",
    "wordID in the forward index and the docID in the inverted\n",
    "index. This limits it to 8 and 5 bits respectively . If the length is longer than would fit in that many\n",
    "bits, an escape code is used in those bits, and the next two\n",
    "bytes contain the actual length. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Crawling the Web**\n",
    "Running a web crawler is a challenging task. There are tricky performance and reliability issues and\n",
    "even more importantly, there are social issues. Crawling is the most fragile application since it involves\n",
    "interacting with hundreds of thousands of web servers and various name servers which are all beyond\n",
    "the control of the system.\n",
    "In order to scale to hundreds of millions of web pages, Google has a fast distributed crawling system. A\n",
    "single **URLserver** serves **lists of URLs** to a number of crawlers.Each crawler keeps roughly 300 connections\n",
    "open at once. This is necessary to retrieve web pages at a fast enough pace. At peak speeds, the system\n",
    "can crawl over 100 web pages per second using four crawlers. This amounts to roughly 600K per second\n",
    "of data. A major performance stress is **DNS** lookup. Each crawler maintains a its own **DNS cache** so it\n",
    "does not need to do a DNS lookup before crawling each document. Each of the hundreds of connections\n",
    "can be in a number of different states: **looking up DNS**, **connecting to host**, **sending request**, and\n",
    "**receiving response**. These factors make the crawler a complex component of the system. It uses\n",
    "asynchronous IO to manage events, and a number of queues to move page fetches from state to state.\n",
    "It turns out that running a crawler which connects to more than half a million servers, and generates tens\n",
    "of millions of log entries generates a fair amount of email and phone calls. Because of the vast number\n",
    "of people coming on line, there are always those who do not know what a crawler is, because this is the\n",
    "first one they have seen. Almost daily, we receive an email something like, \"Wow, you looked at a lot of\n",
    "pages from my web site. How did you like it?\" There are also some people who do not know about the\n",
    "robots exclusion protocol, and think their page should be protected from indexing by a statement like,\n",
    "\"This page is copyrighted and should not be indexed\", which needless to say is difficult for web crawlers\n",
    "to understand. \n",
    "\n",
    "\n",
    "Also, because of the huge amount of data involved, unexpected things will happen. Because of the immense variation in web pages and\n",
    "servers, it is virtually impossible to test a crawler without running it on large part of the Internet.\n",
    "Invariably, there are hundreds of obscure problems which may only occur on one page out of the whole\n",
    "web and cause the crawler to crash, or worse, cause unpredictable or incorrect behavior. Systems which\n",
    "access large parts of the Internet need to be designed to be very robust and carefully tested. Since large\n",
    "complex systems such as crawlers will invariably cause problems, there needs to be significant resources\n",
    "devoted to reading the email and solving these problems as they come up.\n",
    "\n",
    "#### **Indexing The Web**\n",
    "\n",
    "- **Parsing --** Any parser which is designed to run on the entire Web must handle a huge array of\n",
    "possible errors. These range from typos in HTML tags to kilobytes of zeros in the middle of a tag,\n",
    "non-ASCII characters, HTML tags nested hundreds deep, and a great variety of other errors that\n",
    "challenge anyone’s imagination to come up with equally creative ones. For maximum speed they use flex to generate a lexical analyzer which\n",
    "we outfit with its own stack. Developing this parser which runs at a reasonable speed and is very\n",
    "robust involved a fair amount of work. \n",
    "- **Indexing--** After each document is parsed, it is encoded into a number\n",
    "of barrels. Every word is converted into a wordID by using an in-memory hash table -- the lexicon.\n",
    "New additions to the lexicon hash table are logged to a file. Once the words are converted into\n",
    "wordID’s, their occurrences in the current document are translated into hit lists and are written into\n",
    "the forward barrels. The main difficulty with parallelization of the indexing phase is that the\n",
    "lexicon needs to be shared. Instead of sharing the lexicon, they took the approach of writing a log of\n",
    "all the extra words that were not in a base lexicon, which they fixed at 14 million words. That way\n",
    "multiple indexers can run in parallel and then the small log file of extra words can be processed by\n",
    "one final indexer.\n",
    "- **Sorting --** In order to generate the inverted index, the sorter takes each of the forward barrels and\n",
    "sorts it by wordID to produce an inverted barrel for title and anchor hits and a full text inverted\n",
    "barrel. This process happens one barrel at a time, thus requiring little temporary storage. They\n",
    "parallelize the sorting phase to use as many machines as they have simply by running multiple\n",
    "sorters, which can process different buckets at the same time. Since the barrels don’t fit into main\n",
    "memory, the sorter further subdivides them into baskets which do fit into memory based on\n",
    "**wordID** and **docID**. Then the sorter, loads each basket into memory, sorts it and writes its contents\n",
    "into the short inverted barrel and the full inverted barrel.\n",
    "\n",
    "#### **Searching**\n",
    "The goal of searching is to provide quality search results efficiently. Many of the large commercial\n",
    "search engines seemed to have made great progress in terms of efficiency. Therefore, they have focused\n",
    "more on quality of search in their research, although they believe their solutions are scalable to commercial\n",
    "volumes with a bit more effort.\n",
    "\n",
    "*Google Query Evaluetion*\n",
    "\n",
    "- 1. Parse the query.\n",
    "\n",
    "- 2. Convert words into wordIDs.\n",
    "\n",
    "- 3. Seek to the start of the doclist in\n",
    "the short barrel for every word.\n",
    "\n",
    "- 4. Scan through the doclists until\n",
    "there is a document that matches\n",
    "all the search terms.\n",
    "\n",
    "- 5. Compute the rank of that\n",
    "document for the query.\n",
    "\n",
    "- 6. If we are in the short barrels and at\n",
    "the end of any doclist, seek to the\n",
    "start of the doclist in the full barrel\n",
    "for every word and go to step 4.\n",
    "\n",
    "- 7. If we are not at the end of any\n",
    "doclist go to step 4.\n",
    "Sort the documents that have\n",
    "matched by rank and return the top k.\n",
    "\n",
    "\n",
    "#### **Ranking System**\n",
    "Google maintains much more information about web\n",
    "documents than typical search engines. Every hitlist\n",
    "includes position, font, and capitalization information.\n",
    "Additionally, they factor in hits from anchor text and the\n",
    "PageRank of the document. Combining all of this\n",
    "information into a rank is difficult. They designed their ranking\n",
    "function so that no particular factor can have too much\n",
    "influence.\n",
    "\n",
    "\n",
    "Google considers each hit to be one of several different types (**title**, **anchor**, **URL**, **plain text**, **large font**,\n",
    "**plain text small font**, **...**), each of which has its own type-weight. The type-weights make up a vector\n",
    "indexed by type. Google counts the number of hits of each type in the hit list. Then every count is\n",
    "converted into a count-weight. Count-weights increase linearly with counts at first but quickly taper off\n",
    "so that more than a certain count will not help.\n",
    "\n",
    "\n",
    "They take the dot product of the vector of count-weights\n",
    "with the vector of type-weights to compute an IR score for the document. Finally, the **IR score** is\n",
    "combined with **PageRank** to give a **final rank** to the document.\n",
    "\n",
    "\n",
    "For a multi-word search, the situation is more complicated. Now multiple hit lists must be scanned\n",
    "through at once so that hits occurring close together in a document are weighted higher than hits\n",
    "occurring far apart. The hits from the multiple hit lists are matched up so that nearby hits are matched\n",
    "together. For every matched set of hits, a proximity is computed. The proximity is based on how far\n",
    "apart the hits are in the document (or anchor) but is classified into 10 different value **bins** ranging from\n",
    "a phrase match to **not even close**. Counts are computed not only for every type of hit but for every type\n",
    "and proximity. Every type and proximity pair has a type-prox-weight. The counts are converted into\n",
    "count-weights and we take the dot product of the count-weights and the type-prox-weights to compute\n",
    "an IR score. All of these numbers and matrices can all be displayed with the search results using a\n",
    "special debug mode. These displays have been very helpful in developing the ranking system. \n",
    "\n",
    "#### **Feedback**\n",
    "The ranking function has many parameters like the type-weights and the type-prox-weights. Figuring out\n",
    "the right values for these parameters is something of a black art. In order to do this, we have a user\n",
    "feedback mechanism in the search engine. A trusted user may optionally evaluate all of the results that\n",
    "are returned. This feedback is saved. Then when we modify the ranking function, we can see the impact\n",
    "of this change on all previous searches which were ranked. Although far from perfect, this gives us some idea of how a change in the ranking function affects the search results.\n",
    "\n",
    "#### **Results and Performance**\n",
    "The most important measure of a search\n",
    "engine is the quality of its search results.\n",
    "While a complete user evaluation is\n",
    "beyond the scope of this paper, their own\n",
    "experience with Google has shown it to\n",
    "produce better results than the major\n",
    "commercial search engines for most\n",
    "searches. \n",
    "All results are reasonably high\n",
    "quality pages and, at last check, none\n",
    "were broken links. This is largely because\n",
    "they all have high **PageRank**. Of\n",
    "course a true test of the quality of a search engine would involve an extensive user study or results\n",
    "analysis.\n",
    "\n",
    "#### **Search Performance**\n",
    "\n",
    "Current version of Google answers most queries in between 1 and 10 seconds. Furthermore,\n",
    "Google does not have any optimizations such as query caching, subindices on common terms, and other\n",
    "common optimizations. They intend to speed up Google considerably through distribution and hardware,\n",
    "software, and algorithmic improvements.\n",
    "\n",
    "Google is designed to be a scalable search engine.\n",
    "The primary goal is to provide high quality search\n",
    "results over a rapidly growing World Wide Web.\n",
    "Google employs a number of techniques to improve\n",
    "search quality including **page rank**, **anchor text**, and\n",
    "**proximity information**.Google is a\n",
    "complete architecture for gathering web pages,\n",
    "indexing them, and performing search queries over\n",
    "them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768a25d9-444c-46f0-932a-84b150ccd08b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528241c7-3a25-4b29-81f6-7ac19d3622d0",
   "metadata": {},
   "source": [
    "## **Needed Math Theory**\n",
    "\n",
    "\n",
    "What is fascinating with the PageRank algorithm is how to start from a complex problem and end up with a very simple solution.  I will give the idea and theory behind the **PageRank** algorithm.\n",
    "\n",
    "### **Random Walk**\n",
    "The web can be represented like a directed graph where nodes represent the web pages and edges form links between them. Typically, if a node (web page) **i** is linked to a node **j**, it means that **i** refers to **j**.\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://iq.opengenus.org/content/images/2020/05/IMG_0023.jpg\" alt=\"Directed Graph\" width=\"300\" height=\"200\">\n",
    "</center>\n",
    "\n",
    "\n",
    "We have to define what is the importance of a web page. As a first approach, we could say that it is the total number of web pages that refer to it. If we stop to this criteria, the importance of these web pages that refer to it is not taken into account. In other words, an important web page and a less important one has the same weight. Another approach is to assume that a web page spread its importance equally to all web pages it links to.By doing that, we can then define the score  of a node **j** as follows:\n",
    "\n",
    "$$r_j = \\sum_{i \\mapsto j}\\frac{r_i}{di} $$\n",
    "\n",
    "where $r_i$ is the score of the node **i** and $d_i$ its out-degree.\n",
    "\n",
    "\n",
    "From the example above, we can write this linear system:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "r_0 = \\frac{r_4}{3} \\\\\n",
    "r_1 = \\frac{r_2}{2} + \\frac{r_4}{3} + r_3 \\\\\n",
    "r_2 = \\frac{r_0}{3} + \\frac{r_4}{3} \\\\\n",
    "r_3 = \\frac{r_2}{2} + \\frac{r_0}{3} \\\\\n",
    "r_4 = \\frac{r_0}{3} + r_1 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "By passing the right-hand side of this linear system into the left-hand side, we get a new linear system that we can solve by using Gaussian elimination. But this solution is limited for small graphs. Indeed, as this kind of graphs are sparse and Gauss elimination modifies the matrix when performing its operations, we lose the sparsity of the matrix and it would take more memory space. In the worst case, the matrix can no longer be stored.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Damping Factor**\n",
    "The **PageRank theory** holds that an imaginary surfer who is randomly clicking on links will eventually stop clicking. The probability, at any step, that the person will continue following links is a damping factor **d**. The probability that they instead jump to any random page is **1 - d**. Various studies have tested different damping factors, but it is generally assumed that the damping factor will be set around **0.85**.\n",
    "\n",
    "---\n",
    "\n",
    "### **What Is an Adjacency Matrix?**\n",
    "An **adjacency matrix** is a mathematical representation of a graph. It tells you which nodes (web pages, in the case of PageRank) are connected to each others.It’s purely structural — just records the presence of links.\n",
    "For a graph with **𝑛** nodes, the adjacency matrix **𝐴** is an **𝑛×𝑛**matrix where:\n",
    "\n",
    "$ A_{ij} = 1$ if there **is a link** from node **i** to node **j**\n",
    "\n",
    "\n",
    "$ A_{ij} = 0$ if there **is no link** from node **i** to node **j**\n",
    "\n",
    "Let's give some simple example for good understanding ->\n",
    "\n",
    "Say we have a graph with 3 nodes:\n",
    "\n",
    "- **Page A** links to **B**\n",
    "\n",
    "- **Page B** links to **C**\n",
    "\n",
    "- **Page C** links to **A**\n",
    "\n",
    "Then the **adjacency matrix 𝐴**  would be:\n",
    "\n",
    "\n",
    "$$ A = \n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 \\\\\n",
    "1 & 0 & 0\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- Row 1: **Page A** → **Page B**\n",
    "\n",
    "- Row 2: **Page B** → **Page C**\n",
    "\n",
    "- Row 3: **Page C** → **Page A**\n",
    "\n",
    "### **How Adjacency Matrix change to Transition Probability Matrix?**\n",
    "\n",
    "We can’t use the raw **adjacency matrix** for PageRank directly. We must transform it into a **stochastic matrix** a matrix where:\n",
    "\n",
    "- Each column sums to 1\n",
    "\n",
    "- It represents probabilities of moving from one page to another\n",
    "\n",
    "$$ M_{ij} =\n",
    "\\begin{cases}\n",
    "\\frac{1}{outer\\space links}, & \\text{if page j links to page i }\\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases} $$\n",
    "\n",
    "\n",
    "\n",
    "$$ M =\n",
    "\\begin{bmatrix}\n",
    "M_{11} & M_{12} & \\cdots & M_{1n} \\\\\n",
    "M_{21} & M_{22} & \\cdots & M_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "M_{n1} & M_{n2} & \\cdots & M_{nn}\n",
    "\\end{bmatrix}$$ \n",
    "\n",
    "---\n",
    "\n",
    "### **What is eigenvector?**\n",
    "\n",
    "An eigenvector is a special kind of vector in linear algebra. When you multiply a matrix by an eigenvector, the result is just a scaled version of the same vector—not a different direction. This scaling factor is called an **eigenvalue**.\n",
    "\n",
    "$$ A \\cdot \\vec{v} = \\lambda \\cdot \\vec{v} $$\n",
    "\n",
    "- **A** is a square matrix\n",
    "- **v** is an eigenvector\n",
    "- **λ** is the eigenvalue\n",
    "\n",
    "### **How these help in our situation?**\n",
    "In the PageRank algorithm, you represent the web as a **transition matrix**-**M**, \n",
    "where how we say each entry represents the probability of going from one page to another.\n",
    "\n",
    "Then you try to find a **vector of PageRanks** that satisfies this equation:\n",
    "\n",
    "$$ M \\cdot \\vec{r} = \\vec{r} $$\n",
    "\n",
    "\n",
    "$$ M \\cdot \\vec{r} = 1 \\cdot \\vec{r} $$\n",
    "\n",
    "---\n",
    "\n",
    "This means:\n",
    "**The PageRank** vector is an eigenvector of the matrix **𝑀**.\n",
    "**The corresponding eigenvalue is 1**\n",
    "$ \\vec{r}$ represents the steady-state distribution of the random surfer.\n",
    "\n",
    "After many steps, the probability of the surfer being on each page stabilizes\n",
    "\n",
    "That stabilized vector is the **eigenvector with eigenvalue 1** — and those are your **PageRank scores**\n",
    "\n",
    "## **Markov Chains**\n",
    "\n",
    "A **Markov Chain** is a mathematical model describing a system that moves between different **states** with certain **probabilities**. The key characteristic is the **Markov property**:  \n",
    "**The next state depends only on the current state, not on the sequence of previous states.**\n",
    "\n",
    "In the context of web browsing, imagine a user clicking links from one page to another. They don’t remember how they got to the current page—they only consider their **current location** when deciding where to go next.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- **States**: Represent individual web pages.\n",
    "- **Transitions**: Represent links between pages.\n",
    "\n",
    "- **Transition Probability**: The likelihood of moving from one page to another. This includes the **damping factor** to handle random jumps.\n",
    "- **Transition Matrix** \\( P \\): A matrix where each column represents the probabilities of moving from a given page to others.\n",
    "\n",
    "Example of a simple transition matrix:\n",
    "\n",
    "$$\n",
    "M = \n",
    "\\begin{bmatrix}\n",
    "0 & 0.5 & 0 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "0 & 0.5 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Each column sums to 1, which is a requirement for **stochastic matrices** used in Markov Chains.\n",
    "\n",
    "---\n",
    "\n",
    "Transition Matrix in Practice:\n",
    "\n",
    "$$\n",
    "P = \\left(\\begin{array}{cc}\n",
    "0 & 0 & 0 & 0 & \\frac{1}{3} \\\\\n",
    "0 & 0 & \\frac{1}{2} & 1 & \\frac{1}{3} \\\\\n",
    "\\frac{1}{3} & 0 & 0 & 0 &  \\frac{1}{3} \\\\\n",
    "\\frac{1}{3} & 0 & \\frac{1}{2} & 0 & 0 \\\\\n",
    "\\frac{1}{3} & 1 & 0 & 0 & 0\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "This matrix is **column-stochastic**, and its **transpose** is **row-stochastic**, satisfying the conditions to apply **Markov chain theorems**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Let the initial probability distribution be:\n",
    "\n",
    "$$\n",
    "\\pi^{(0)} = \\left( \\frac{1}{n}, \\frac{1}{n}, \\ldots, \\frac{1}{n} \\right)\n",
    "$$\n",
    "\n",
    "At each step:\n",
    "\n",
    "$$\n",
    "\\pi^{(t+1)} = P \\cdot \\pi^{(t)}\n",
    "$$\n",
    "\n",
    "The distribution evolves until it **converges** to a **stationary distribution** $\\pi$, where:\n",
    "\n",
    "$$\n",
    "P \\cdot \\pi = \\pi\n",
    "$$\n",
    "\n",
    "This represents the long-term probability of being on each page. If the graph is **strongly connected** (every node is reachable from every other), a stationary distribution **always exists**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Solving $ P \\cdot \\pi = \\pi$ is equivalent to finding the **eigenvector** of matrix $ P $ with eigenvalue 1. According to the **Frobenius-Perron Theorem**:\n",
    "\n",
    "> If $ P $ is a **positive** and **square** matrix, then it has a unique dominant eigenvector with all positive entries.\n",
    "\n",
    "In our case:\n",
    "-  $P$ is a positive matrix (after applying damping).\n",
    "- The dominant eigenvector $\\pi$ corresponds to the **PageRank scores**.\n",
    "- The entries of $\\pi$ indicate the **relative importance** of each page.\n",
    "\n",
    "---\n",
    "\n",
    "We use the **power iteration method**:\n",
    "\n",
    "1. Start with an initial guess $ \\pi^{(0)} $\n",
    "2. Update using $ \\pi^{(t+1)} = P \\cdot \\pi^{(t)} $\n",
    "3. Repeat until convergence\n",
    "\n",
    "After convergence, the vector $ \\pi $ gives the **ranking** of web pages.\n",
    "\n",
    "The higher the value of $ \\pi_i $, the more important the page $ i $ is in the network.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Teleportation**\n",
    "\n",
    "In the **web graph**, for example, we can find a web page **i** which refers only to web page **j** and **j** refers only to **i**. This is what we call **spider trap problem**. We can also find a web page which has no outlink. It is commonly named **Dead end**.\n",
    "\n",
    "In the case of a **spider trap**, when the **random walker** reaches the **node 1*8 in the above example, he can only jump to **node 2** and from **node 2**, he can only reach **node 1**, and so on. The importance of all other nodes will be taken by **nodes 1** and **2**. In the above example, the probability distribution will converge to $\\pi = (0, 0.5, 0.5, 0)$. This is not the desired result.\n",
    "\n",
    "In the case of **Dead ends**, when the walker arrives at **node 2**, it can’t reach any other node because it has no outlink. The algorithm cannot converge.\n",
    "\n",
    "To get over these two problems, we introduce the notion of **teleportation**.\n",
    "\n",
    "**Teleportation** consists of connecting each node of the graph to all other nodes. The graph will be then complete. The idea is with a certain probability $\\beta$, the **random walker** will jump to another node according to the **transition matrix P** and with a probability **(1-β)/n**, it will jump randomly to any node in the graph. We get then the new **transition matrix R**\n",
    "\n",
    "\n",
    "$$ R = \\beta P +(1-\\beta)ve^T$$\n",
    "\n",
    "where **v** is a vector of ones, and **e** a vector of **1/n**:\n",
    "\n",
    "$$ e^T = (\\frac{1}{n},\\cdots,\\frac{1}{n}) $$\n",
    "\n",
    "\n",
    "$$ v = (1,\\cdots,1)^T $$\n",
    "\n",
    "$\\beta$ is commonly defined as the **damping factor**\n",
    "\n",
    "\n",
    "The  **matrix R** has the same properties than **P** which means that it admits a stationary distribution, so we can use all the theorems we saw previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb54b5-ee41-4c71-8fb7-0b30c364005d",
   "metadata": {},
   "source": [
    "# **Algorithms**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a59522d-ddd7-4af1-92f4-cd5bb1fdeed0",
   "metadata": {},
   "source": [
    "## **PageRank**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a3760-f2af-48f4-8642-aef46ae13bc5",
   "metadata": {},
   "source": [
    "**PageRank** was created to tacke the difficulty of determing the importance of web pages with immense amount of information available.Demonstrates how pure math can have massive practical impact — affecting **billions of users** every day.The purpose of the algorithm is to provide better search results that are more precise and related by taking into account various factors beyond just matching keywords.How i mentonied before most search engines focused only on matching words, not on evaluating the importance of pages.PageRank introduced a completely new way of thinking about web pages. Instead of treating every page as equal, it looked at how other pages linked to it.\n",
    "A page is important if important pages link to it.\n",
    "How before we told that **PageRank** algorithm is based on the idea of a **random surfer** — someone who starts on a web page and randomly follows links.\n",
    "Sometimes they click a link on the page or get bored and jump to a random new page.\n",
    "By simulating this behavior across the web, PageRank calculates the steady-state probability of landing on any given page. That value becomes the page’s ranking score.\n",
    "\n",
    "<center>\n",
    "    <img src=\"https://th.bing.com/th/id/OIP.gazojYMdRdFHxk9FqT3OQAHaF9?r=0&rs=1&pid=ImgDetMain\" alt=\"PageRank\" width=\"300\" height=\"200\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8066b26f-547e-4af5-ac4b-197001a639e6",
   "metadata": {},
   "source": [
    "## **Basic Idea**\n",
    "To illustrate how **PageRank** works i will show simple way of explanation.\n",
    "\n",
    "Let's use players in a football match:\n",
    "\n",
    "- each player represent a page\n",
    "\n",
    "- each pass between two players represent a link between 2 pages\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0033368d-9206-42d0-bacc-69346adf18ba",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://thumbs.dreamstime.com/b/vector-soccer-field-arrangement-players-game-position-title-football-player-green-field-template-vector-119751372.jpg\" alt=\"Image of football field with red points for player\" width=\"300\" height=\"200\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7350f0dd-a1cb-48af-8aca-09ffafe1437d",
   "metadata": {},
   "source": [
    "The main thing PageRank uses are:\n",
    "  - The number of links the page gets(in football context how many pass player recieves).\n",
    "    \n",
    "  - The importance of a page is determined by the number of links pointing towards it by how frequently the player who passed the ball is passed to.\n",
    "The **PageRank** of each player gets updated every time they receive the ball."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ffbec3-2940-49ca-9243-5579a0793c73",
   "metadata": {},
   "source": [
    "- As more passes are made ,the PageRank of each player undergoes changes.\n",
    "\n",
    "- As a result ,the **PageRank** of every player they pass will be altered.\n",
    "\n",
    "- **The higher number it's better**.\n",
    "\n",
    "- Once game concludes,players can be **sorted** by their rating and be **ranked** to determine the best performer!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101588b8-7396-41ad-ab81-947c69b4f388",
   "metadata": {},
   "source": [
    "As this very simple explanation we hover the main idea of **PageRank Algorithm**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817d67e7-cdbd-4b9e-b06b-a867f1cadd57",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6bada4-67f6-4390-b44a-f39ad3cc79df",
   "metadata": {},
   "source": [
    "## **Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b43f4b-0792-4804-b350-ec440ddfad4b",
   "metadata": {},
   "source": [
    "The PageRank algorithm outputs a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page. PageRank can be calculated for collections of documents of any size. It is assumed in several research papers that the distribution is evenly divided among all documents in the collection at the beginning of the computational process. The PageRank computations require several passes, called **iterations**,through the collection to adjust approximate PageRank values to more closely reflect the theoretical true value.\n",
    "\n",
    "A probability is expressed as a numeric value between 0 and 1. A 0.5 probability is commonly expressed as a \"50% chance\" of something happening. Hence, a document with a PageRank of 0.5 means there is a 50% chance that a person clicking on a random link will be directed to said document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c662d5-f4d4-47df-b877-70dbb30c791d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e84f4c1-fe11-4b51-ba8e-08fd434564bc",
   "metadata": {},
   "source": [
    "Lets have small universe of four web pages: **A**, **B**, **C**, and **D**. Links from a page to itself are ignored. Multiple outbound links from one page to another page are treated as a single link. **PageRank** is initialized to the same value for all pages. In the original form of **PageRank**, the sum of **PageRank** over all pages is the total number of pages on the web at that time.However, later versions of **PageRank**, and the remainder of this section, assume a probability distribution between **0** and **1**. Hence the initial value for each page in this example is **0.25**.\n",
    "\n",
    "The PageRank transferred from a given page to the targets of its outbound links upon the next iteration is divided equally among all outbound links.\n",
    "\n",
    "If the only links in the system were from pages **B**, **C**, and **D** to **A**, each link would transfer **0.25** PageRank to **A** upon the next iteration, for a total of 0.25 + 0.25 + 0.25 = **0.75**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e6ef2-e6d8-4ba7-aa99-768abcc1137e",
   "metadata": {},
   "source": [
    "$$ PR(A) = PR(B) + PR(C) + PR(D). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47273de0-d911-4292-b90c-f429bac4145d",
   "metadata": {},
   "source": [
    "Instead in our example page **B** had a link to pages **C** and **A**, page **C** had a link to page **A**, and page **D** had links to all three pages. In the first iteration, page **B** would transfer half of its existing value **(0.125)** to page **A** and the other half **(0.125)** to page **C**. Page **C** would transfer all of its existing value **(0.25)** to the only page it links to, **A**. Since **D** had three outbound links, it would transfer one third of its existing value, or approximately **0.083**, to **A**. At the completion of this iteration, page **A** will have a PageRank of approximately **0.458**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4bcea4-4012-4345-8ebb-a7b12d8ab0fb",
   "metadata": {},
   "source": [
    "$$ PR(A) = \\frac{PR(B)}{2} + \\frac{PR(C)}{1} + \\frac{PR(D)}{3}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e42b98-f04d-4ac2-aa1c-f9a9775f3685",
   "metadata": {},
   "source": [
    "In other words, the PageRank conferred by an outbound link is equal to the document's own PageRank score divided by the number of outbound links **L( )**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037031a-715d-43c9-ab95-2358be1c0918",
   "metadata": {},
   "source": [
    "$$ PR(A) = \\frac{PR(B)}{L(B)} + \\frac{PR(C)}{L(C)} + \\frac{PR(D)}{L(D)}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbfaaab-ec37-4a20-957b-0bc98889ac20",
   "metadata": {},
   "source": [
    "In general case PageRank value for a page **U** is dependent on the PageRank values for each page **V** contained in the set **Bu** (the set containing all pages linking to page **U**), divided by the number **L(V)** of links from page **V**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c2256b-7bbb-4620-8722-c2f20d40a222",
   "metadata": {},
   "source": [
    "$$ PR(U) = \\sum_{v \\in B_u} \\frac{PR(V)}{L(V)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aee6a92-0362-4b3e-8b59-624241c640c5",
   "metadata": {},
   "source": [
    "The damping factor is subtracted from 1 (and in some variations of the algorithm, the result is divided by the number of documents (N) in the collection) and this term is then added to the product of the damping factor and the sum of the incoming PageRank scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431daad6-4a07-4389-9f68-c75249d08a7f",
   "metadata": {},
   "source": [
    "$$  PR(A) = \\frac{1-d}{N} + d\\left(\\frac{PR(B)}{L(B)} + \\frac{PR(C)}{L(C)} + \\frac{PR(D)}{L(D)} + \\cdots \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dc11db-196a-436b-894d-f91d9e632a7a",
   "metadata": {},
   "source": [
    "The damping factor adjusts the derived value downward. The original paper, however, gave the following formula, which has led to some confusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02683afd-ea22-4916-b730-9e899a5d694e",
   "metadata": {},
   "source": [
    "$$  PR(A) = 1-d + d\\left(\\frac{PR(B)}{L(B)} + \\frac{PR(C)}{L(C)} + \\frac{PR(D)}{L(D)} + \\cdots \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca04b46-1bc9-47b4-8980-f8825fc25013",
   "metadata": {},
   "source": [
    "The difference between them is that the PageRank values in the first formula sum to one, while in the second formula each PageRank is multiplied by N and the sum becomes N. A statement in Page and Brin's paper that the sum of all PageRanks is one and claims by other Google employees support the first variant of the formula above.\n",
    "\n",
    "Page and Brin confused the two formulas in their most popular paper \"The Anatomy of a Large-Scale Hypertextual Web Search Engine\", where they mistakenly claimed that the second formula formed a probability distribution over web pages.\n",
    "\n",
    "Google recalculates PageRank scores each time it crawls the Web and rebuilds its index. As Google increases the number of documents in its collection, the initial approximation of PageRank decreases for all documents.\n",
    "\n",
    "The formula uses a model of a random surfer who reaches their target site after several clicks, then switches to a random page. The PageRank value of a page reflects the chance that the random surfer will land on that page by clicking on a link. It can be understood as a **Markov chain** in which the **states** are **pages**, and the **transitions** are the **links** between pages – all of which are all equally probable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc3f51b-c331-4226-8f6e-7c32be1cf12c",
   "metadata": {},
   "source": [
    "---\n",
    "When calculating PageRank, pages with no outbound links are assumed to link out to all other pages in the collection. Their PageRank scores are therefore divided evenly among all other pages. In other words, to be fair with pages that are not sinks, these random transitions are added to all nodes in the Web. This residual probability, **d**, is usually set to **0.85**, estimated from the frequency that an average surfer uses his or her browser's bookmark feature. So, the equation is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f047d-2c3d-44f7-b009-6734a37bf317",
   "metadata": {},
   "source": [
    "$$  PR(pi\\text) = \\frac{1-d}{N} \\sum_{pj \\in M(pi\\text)} \\frac{PR(pi\\text)}{L(pi\\text)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd55e1a-e6b9-4b47-a424-001f61147a30",
   "metadata": {},
   "source": [
    "where $ p_1,p_2,...,p_n$ are pages under consideration,$ M(pi\\text)$ is the set of pages that link $ pi\\text ,L(pi\\text)$ is the number of outbond links on page $ p_j$ , and **N** is the total number of pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d26cfa-6955-46dd-8ffe-c648f60865bf",
   "metadata": {},
   "source": [
    "The PageRank values are the entries of the dominant right **eigenvector** of the modified adjacency matrix rescaled so that each column adds up to one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a73f0d-e3d3-40fb-866e-fb7b58b2a566",
   "metadata": {},
   "source": [
    "So this is the main formula of  **PageRank** algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d44da07-1bc4-45cb-8a2c-5ff901e4e584",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf94c7-7a1f-483e-9ead-04a3ce929998",
   "metadata": {},
   "source": [
    "The **PageRank** values are the entries of the dominant right eigenvector of the **modified adjacency matrix** rescaled so that each column adds up to one. \n",
    "This makes PageRank a particularly elegant metric: the eigenvector is\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b2bb7d-08f3-447f-89aa-5c4beddcc35b",
   "metadata": {},
   "source": [
    "$$ R = \\begin{bmatrix}\n",
    "PR(p_1) \\\\\n",
    "PR(p_2)  \\\\\n",
    "\\vdots  \\\\\n",
    "PR(p_N)\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b110da15-a80f-47a2-9d77-3ff8056db864",
   "metadata": {},
   "source": [
    "where **R** is the solution of the equtation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a0b2f5-745b-42fa-9b67-496e93186957",
   "metadata": {},
   "source": [
    "$$ R=\\begin{bmatrix}\n",
    "(d-1)/N \\\\\n",
    "(d-1)/N \\\\\n",
    "\\vdots  \\\\\n",
    "(d-1)/N\n",
    "\\end{bmatrix} + d\\begin{bmatrix}\\ell(p_1,p_1) & \\ell(p_1,p_2) & \\cdots &  \\ell(p_1,p_N)\\\\ \n",
    "\\ell(p_2,p_1) & \\ddots &  \\space & \\vdots  \\\\\n",
    "\\vdots & \\space & \\ell(p_i,p_j) & \\space \\\\\n",
    "\\ell(p_N,p_1) & \\cdots & \\space & \\ell(p_N,p_N)  \\end{bmatrix}R$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b8bd0a-21c8-4a8d-8146-a0fd264d28d8",
   "metadata": {},
   "source": [
    "where adjency function $ \\ell(p_i,p_j)$ is the ratio between number of links outbound from page **j** to page **i** to the total number of outbound links of the page **j**.The adjency function is 0 if page $p_j$ does not link $ p_i$,and normalized that for each j:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc46c1-6b51-4070-a75d-68d5f6cb781e",
   "metadata": {},
   "source": [
    "$$ \\sum_{i=0}^{N} \\ell(p_i,p_j) = 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb22225-1015-4616-8e4b-278ad525a482",
   "metadata": {},
   "source": [
    "Elements of each columns is sum to **1**,so matrix is a **stochastic metrix**.It's a variant of **eigenvector** centrality measure used commonly in network analysis.Because of the large **eigengap** of the modified adjacency matrix above,the values of the PageRank eigenvector can be approximated to within a high degree of accuracy within only a few iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d68321-6a6a-4e93-acfe-16be0773aa62",
   "metadata": {},
   "source": [
    "Through this data, algorithm can be scaled very well and that the scaling factor for extremely large networks would be roughly linear in \n",
    "$\\log n$ where **n** is the size of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fa335f-bd5f-47d4-9bf0-73e18d6bb017",
   "metadata": {},
   "source": [
    "## Computation Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedbb27f-d865-47ff-92a3-f14159230d53",
   "metadata": {},
   "source": [
    "At $ t = 0 $ ,an initial probability distribution is asummed usually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bcdb3e-0ebe-4c8c-b9d9-357150e4510f",
   "metadata": {},
   "source": [
    "$$ PR(pi;0) = \\frac{1}{N}$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003c6b6-f457-4eee-b6a4-80b53ddad937",
   "metadata": {},
   "source": [
    "where **N** is the total number of pages and $pi;0$ is a page $ i$ at time 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878c14ff-d46d-49ca-a921-5aba63c68c64",
   "metadata": {},
   "source": [
    "At each time step,the computation,as detailed above yields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ea5745-f493-48a8-a31d-7771eb3fd12b",
   "metadata": {},
   "source": [
    "$$ PR(pi;t+1) = \\frac{1-d}{N} + d  \\sum_{pj \\in M(pi\\text)} \\frac{PR(pj;t)}{L(pj)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9097647f-11b0-4aab-9a0d-44db3b3f3a61",
   "metadata": {},
   "source": [
    "where **d** is dumping factor or in matrix notation **->**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452a269e-304f-4162-bd07-ad02e46e483c",
   "metadata": {},
   "source": [
    "$$ R(t + 1) = dMR(t)+\\frac{1-d}{N}1$$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfbb619-8f51-43bc-a2d7-ec38f5af94e9",
   "metadata": {},
   "source": [
    "where $ R_i(t) = PR(pi;t)$ and **1** is column vector of length **N** containing only ones.\n",
    "\n",
    "The Matrix **M** is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fc4eab-aec1-4efc-aaaa-5912aa143f5b",
   "metadata": {},
   "source": [
    "$$ M_{ij} =\n",
    "\\begin{cases}\n",
    "{1}/{L(p_j)}, & \\text{if j links to i } \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases} $$\n",
    "\n",
    "Same as \n",
    "\n",
    "$$  M = (K^{-1}A)^T$$\n",
    "\n",
    "where \n",
    "- **A** denotes the adjacency matrix of the graph\n",
    "- **K** This is a diagonal matrix where each diagonal entry  is the outdegree of page (how many pages it links to).\n",
    "\n",
    "$$\n",
    "K = \n",
    "\\begin{bmatrix}\n",
    "2 & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "But $ K^{-1}$ is inverse of $K$\n",
    "\n",
    "$$\n",
    "K^{-1} = \n",
    "\\begin{bmatrix}\n",
    "\\frac{1}{2} & 0 & 0 \\\\\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We want a transition matrix 𝑀 where 𝑀𝑖𝑗  = probability of moving from page j to page i\n",
    "To get that, we:\n",
    "- Normalize each column of 𝐴 by its outdegree\n",
    "- Transpose to match the direction of movement\n",
    "\n",
    "This creates **transition probability matrix**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6f7eae-df88-4911-82e3-8ffaca6ce78e",
   "metadata": {},
   "source": [
    "The probability calculation is made for each page at a time point, then repeated for the next time point. The computation ends when for some small **ϵ**!\n",
    "$$ |R(t+1) - R(t)| < ϵ  $$\n",
    "This is callled **convergence**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f1f54-9ae9-48dd-a1e9-556db2bd86f5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad92ffe-2060-4c48-9633-a83cae1a5603",
   "metadata": {},
   "source": [
    "##  **Implementation with Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e324b53e-78d4-4d50-b350-6019aab37f62",
   "metadata": {},
   "source": [
    "First I will implement **PageRank algoritm** with simplier way using **networkx** library which has integrate inside **nx.pagerank()** function  with parameter graph and damping factor!After that i will do pure Python **PageRank** algorithm! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31de245-f0ef-4102-b2db-b2ee26a23204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CSV file which is simple web graph of 500 pages with random links\n",
    "df = pd.read_csv(\"simple_web_graph.csv\")\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add edges from the CSV\n",
    "edges = list(zip(df[\"source\"],df[\"target\"]))\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Run PageRank\n",
    "\n",
    "pagerank_scores = nx.pagerank(G,alpha=0.85)\n",
    "\n",
    "# Convert to DataFrame and sort by rank\n",
    "\n",
    "pagerank_df = pd.DataFrame(pagerank_scores.items(), columns=[\"Page\", \"Score\"])\n",
    "pagerank_df = pagerank_df.sort_values(by=\"Score\", ascending=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f2991c-f42f-493f-8c2f-d110b6dc6e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a pages  of the graph with this function!\n",
    "\n",
    "def visulize_graph(G,pagerank_df,pages,title):\n",
    "    small_graph = G.subgraph(list(pagerank_df.head(pages)[\"Page\"]))\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    nx.draw(small_graph, with_labels=True, node_size=700, node_color='lightblue', arrows=True)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0fc382-6a30-4bfd-826c-1d3bfca1aa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visuallise top 10 pages!\n",
    "visulize_graph(G,pagerank_df,10,\"Top 10 pages by PageRank algoritm!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d745ee-8c1b-4f44-940d-9b0aa615b44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visuallise top 30 pages!\n",
    "visulize_graph(G,pagerank_df,30,\"Top 30 pages by PageRank algoritm!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82bd14e-f3d4-40a8-b1e7-3c42b1415f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visuallise 50 pages!\n",
    "visulize_graph(G,pagerank_df,50,\"Top 50 pages by PageRank algoritm!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e91c15-f9dc-4e33-973c-8ddd936afaf5",
   "metadata": {},
   "source": [
    "## PageRank algorithm function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3956979-0b48-46db-932c-1f0add1e2fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(df,alpha=0.85, tol=1e-6, max_iter=100):\n",
    "    pages = sorted(set(df['source']).union(set(df['target'])))\n",
    "    page_to_index = {page: i for i, page in enumerate(pages)}\n",
    "    index_to_page = {i: page for page, i in page_to_index.items()}\n",
    "    n = len(pages)\n",
    "\n",
    "    #Initilize Transition Matrix\n",
    "    M = np.zeros((n,n))\n",
    "\n",
    "    #Fill the matrix based on links\n",
    "    for row in df.itertuples():\n",
    "        src = page_to_index[row.source]\n",
    "        tgt = page_to_index[row.target]\n",
    "        M[tgt,src] += 1 # Column-stochastic matrix\n",
    "\n",
    "    #Hanle Dangling Nodes Too\n",
    "    for i in range(n):\n",
    "     col_sum = M[:, i].sum()\n",
    "     if col_sum != 0:\n",
    "        M[:, i] /= col_sum\n",
    "     else:\n",
    "        # Handle dangling node (pages with no outbound links): use uniform distribution \n",
    "        M[:, i] = 1.0 / n\n",
    "\n",
    "     \n",
    "\n",
    "    matrix_shape = M.shape[0]\n",
    "    v = np.ones(matrix_shape) / matrix_shape # Intitial Rank\n",
    "    teleport = np.ones(matrix_shape) / n  #Teleportation Vector\n",
    "\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        v_new = alpha * np.matmul(M, v) + (1 - alpha) * teleport\n",
    "        if np.linalg.norm(v_new - v, 1) < tol:\n",
    "            break\n",
    "        v = v_new\n",
    "\n",
    "    # Result is a stationary distribution vector — each entry is the PageRank of that page.\n",
    "\n",
    "    return v\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f4e9cf-827e-47a0-bc2d-d5dece5aa98f",
   "metadata": {},
   "source": [
    "### **Compare Results** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c90705a-ecda-4cf3-8107-04de96c012d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pagerank(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dfe7c3-54c3-46ea-a61d-07b74d31e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit nx.pagerank(G,alpha=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6395ed-ded4-44c8-ac07-95ebefbf79f7",
   "metadata": {},
   "source": [
    "### **Damping Factor Change**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e929d7-ac07-4f95-a828-87f45f4ba0dd",
   "metadata": {},
   "source": [
    "**Low damping** → More randomness (less importance to link structure)\n",
    "\n",
    "**High damping** → More importance to actual links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93382409-ccd8-4ef1-a17c-32213632de6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "damping_values = [0.3,0.4,0.5, 0.7, 0.85, 0.95]\n",
    "\n",
    "# Store all PageRank results for each damping factor\n",
    "all_ranks = []\n",
    "\n",
    "# Run pagerank for each damping factor and collect the rank arrays\n",
    "for d in damping_values:\n",
    "    ranks = pagerank(df, alpha=d)  # returns NumPy array\n",
    "    all_ranks.append(ranks)\n",
    "\n",
    "# Convert list of arrays into a 2D NumPy array for easier indexing [damping_index][page_index]\n",
    "rank_matrix = np.array(all_ranks)\n",
    "\n",
    "# Plot PageRank scores across damping values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "num_pages = rank_matrix.shape[1]  # number of pages\n",
    "\n",
    "# Plot for each page\n",
    "for i in range(min(10,num_pages)):\n",
    "    plt.plot(damping_values, rank_matrix[:, i], marker='o', label=f'Page {i}')\n",
    "\n",
    "plt.title(\"PageRank Score vs Damping Factor\")\n",
    "plt.xlabel(\"Damping Factor\")\n",
    "plt.ylabel(\"PageRank Score\")\n",
    "plt.grid(True)\n",
    "\n",
    "# If too many pages, you can comment this line or limit the number of pages plotted\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=\"small\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f8d730-cc7f-434d-8e4c-f69d4550b6e9",
   "metadata": {},
   "source": [
    "## **Personalized PageRank**\n",
    "Personalized PageRank (PPR) is an adaptation of the classic PageRank algorithm that personalizes the ranking results for a specific user, query, or topic. Instead of treating all nodes equally during teleportation, PPR biases the random surfer toward a personalized subset of nodes.\n",
    "\n",
    "In contrast to classic PageRank (where teleportation happens uniformly to any node), PPR allows the teleportation step to prefer specific nodes based on interest, relevance, or preference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0394bb-826b-4531-bab6-7ad8d4da4b31",
   "metadata": {},
   "source": [
    "$$ \n",
    "\\vec{r} = \\alpha M \\vec{r} + (1 - \\alpha) \\vec{v}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7fe62e-65d5-4ac3-8a07-2e1f48b93ced",
   "metadata": {},
   "source": [
    "## **Implementation of Personlized PageRank**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c41cb1-b3f7-4388-be3e-bb0e28decc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def personalized_pagerank(G, alpha=0.85, personalization_node=None, max_iter=100, tol=1e-6):\n",
    "    nodes = list(G.nodes())\n",
    "    N = len(nodes)\n",
    "    A = nx.to_numpy_array(G, nodelist=nodes)\n",
    "    \n",
    "    # Create transition matrix M\n",
    "    M = A / A.sum(axis=1, keepdims=True)\n",
    "    M = np.nan_to_num(M)  # handle dangling nodes\n",
    "    \n",
    "    # Personalization vector v\n",
    "    v = np.zeros(N)\n",
    "    if personalization_node is not None:\n",
    "        index = nodes.index(personalization_node)\n",
    "        v[index] = 1.0\n",
    "    else:\n",
    "        v[:] = 1.0 / N\n",
    "    v = v / v.sum()  # ensure it’s a distribution\n",
    "    \n",
    "    # Initialize pagerank vector\n",
    "    p = np.ones(N) / N\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        p_new = alpha * M.T @ p + (1 - alpha) * v\n",
    "        if np.linalg.norm(p_new - p, 1) < tol:\n",
    "            break\n",
    "        p = p_new\n",
    "    \n",
    "    return dict(zip(nodes, p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd0e6b8-032f-4dff-a32e-471d2e233aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "edges = [(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"C\"), (\"C\", \"A\"), (\"D\", \"C\")]\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "result = personalized_pagerank(G, alpha=0.85, personalization_node=\"A\")\n",
    "print(\"Personalized PageRank starting from node A:\")\n",
    "for node, score in result.items():\n",
    "    print(f\"{node}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd135f-4292-493b-a580-728e696c2734",
   "metadata": {},
   "source": [
    "## **HITS Algorithm(_Hyperlink Induced Topic Search_)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c743485-b1f2-4187-9054-1058cc39e315",
   "metadata": {},
   "source": [
    "**Hyperlink Induced Topic Search (HITS) Algorithm** is a Link Analysis Algorithm that rates webpages, developed by Jon Kleinberg. This algorithm is used to the web link-structures to discover and rank the webpages relevant for a particular search.\n",
    "\n",
    "\n",
    "Unlike **PageRank**, which assigns a single score to each page representing its importance, **HITS** assigns two scores to every page:\n",
    "\n",
    "- **Authority Score**: Measures the value of the content on the page. A good authority page is one that is linked to by many good hubs.\n",
    "\n",
    "- **Hub Score**: Measures the value of the links on the page. A good hub page links to many good authorities.\n",
    "\n",
    "\n",
    "It use hubs and authorities to define a recursive relationship between webpages. \n",
    " \n",
    "- Given a query to a Search Engine, the set of highly relevant web pages are called Roots. They are potential **Authorities**.\n",
    "- Pages that are not very relevant but point to pages in the Root are called **Hubs**. Authority is a page that many hubs link to whereas a Hub is a page that links to many authorities.\n",
    "\n",
    "### **How it works?**\n",
    "\n",
    "**HITS** is based on the idea that good hubs point to good authorities, and good authorities are pointed to by good hubs. The algorithm operates on a subset of the web graph related to a specific query:\n",
    "A small subgraph (called a root set) is first created by collecting the top pages that match the query using a basic search.\n",
    "This set is expanded by including pages that link to or are linked from the root set (forming a base set).\n",
    "The hub and authority scores are then computed iteratively:\n",
    "\n",
    "- Each node's authority score is the sum of the hub scores of the nodes pointing to it.\n",
    "\n",
    "- Each node's hub score is the sum of the authority scores of the nodes it points to.\n",
    "\n",
    "These scores are typically normalized in each iteration, and the process repeats until the scores converge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9052fd-d093-41cd-add5-73092c2de867",
   "metadata": {},
   "source": [
    "## **Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03f05e9-a4c0-429a-9763-e7d121d98feb",
   "metadata": {},
   "source": [
    "Let's have a directed graph $ G = (V,E)$ , where:\n",
    "\n",
    "- $V$ is a set of nodes\n",
    "- $E$ is set of directed edges(hyperlinks)\n",
    "\n",
    "And have **adjacency matrix** representing links between pages.$A \\in \\mathbb{R}^{n×n}$\n",
    "\n",
    "$ A_{ij}=1$ if page i link to page j,else 0.\n",
    "\n",
    "$ h = \\mathbb{R}^{n}$ hub score vector\n",
    "\n",
    "$ a = \\mathbb{R}^{n}$ authority score vector\n",
    "\n",
    "Update rules are:\n",
    "\n",
    "$$ a = A^{T}h $$ (authority is sum of hub scores pointing to it)\n",
    "\n",
    "$$ h = Aa$$ (hub is sum of authority scores it points to)\n",
    "\n",
    "Combining:\n",
    "\n",
    "$$ a^{(k + 1)} = A^{T}Aa^{(k)}$$ \n",
    "\n",
    "\n",
    "$$ h^{(k + 1)} = AA^{(T)}h^{(k)}$$ \n",
    "\n",
    "So: \n",
    "\n",
    "$ A^TA $ is the authority matrix!\n",
    "\n",
    "$ AA^T $ is the hub matrix!\n",
    "\n",
    "These are symmetric positive semi-definite matrices, and the iterative update converges to the principal eigenvector of each matrix:\n",
    "\n",
    "- The principal eigenvector of $A^TA$ gives authority scores.\n",
    "\n",
    "- The principal eigenvector of $AA^T$ gives hub scores.\n",
    "\n",
    "### **What this mean practically?**\n",
    "\n",
    "- You can compute **authority scores** by finding the top eigenvector of $A^{T}A $ .\n",
    "- You can compute **hub scores** by finding the top eigenvector of $ AA^{T}$.\n",
    "\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7cf61-d7fb-49b0-b6df-41fb83d8c0cc",
   "metadata": {},
   "source": [
    "-  Let number of iterations be **k**. \n",
    "-  Each node is assigned a **Hub score = 1** and an **Authority score = 1.**\n",
    "- Repeat **k** times: \n",
    " \n",
    "\n",
    "\n",
    "- **Hub update** : Each node's Hub score = Σ  (Authority score of each node it points to).\n",
    "\n",
    "- **Authority update** : Each node's Authority score = Σ  (Hub score of each node pointing to it).\n",
    "\n",
    "\n",
    "Normalize the scores by dividing each Hub score by square root of the sum of the squares of all Hub scores, and dividing each Authority score by square root of the sum of the squares of all Authority scores. (optional)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846d13a7-2be0-4044-bee4-9078f058d2de",
   "metadata": {},
   "source": [
    "## **Implementation with Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fae4c2-5ec9-4298-a210-14fe38c41768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_algorithm(adj_matrix, max_iter=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Computes hub and authority scores using the HITS algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "        adj_matrix (numpy.ndarray): Adjacency matrix of the graph (shape: n x n)\n",
    "        max_iter (int): Maximum number of iterations\n",
    "        tol (float): Convergence tolerance\n",
    "    \n",
    "    Returns:\n",
    "        hubs (numpy.ndarray): Hub scores\n",
    "        authorities (numpy.ndarray): Authority scores\n",
    "    \"\"\"\n",
    "    n = adj_matrix.shape[0]\n",
    "\n",
    "    # Initialize hub and authority scores to 1\n",
    "    hubs = np.ones(n)\n",
    "    authorities = np.ones(n)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # Update authority scores\n",
    "        new_authorities = adj_matrix.T @ hubs\n",
    "        # Update hub scores\n",
    "        new_hubs = adj_matrix @ new_authorities\n",
    "\n",
    "        # Normalize\n",
    "        new_authorities = new_authorities / np.linalg.norm(new_authorities, 2)\n",
    "        new_hubs = new_hubs / np.linalg.norm(new_hubs, 2)\n",
    "\n",
    "        # Check convergence\n",
    "        if np.allclose(hubs, new_hubs, atol=tol) and np.allclose(authorities, new_authorities, atol=tol):\n",
    "            print(f\"Converged in {i+1} iterations.\")\n",
    "            break\n",
    "\n",
    "        hubs = new_hubs\n",
    "        authorities = new_authorities\n",
    "\n",
    "    return hubs, authorities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242ad772-6aad-4181-82ec-adde83ccc5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example graph (5 pages)\n",
    "A = np.array([\n",
    "    [0, 1, 1, 0, 0],  # Page 0 links to 1, 2\n",
    "    [0, 0, 1, 1, 0],  # Page 1 links to 2, 3\n",
    "    [1, 0, 0, 0, 1],  # Page 2 links to 0, 4\n",
    "    [0, 0, 0, 0, 1],  # Page 3 links to 4\n",
    "    [0, 0, 1, 0, 0]   # Page 4 links to 2\n",
    "])\n",
    "\n",
    "hubs, authorities = hits_algorithm(A)\n",
    "\n",
    "print(\"Hub Scores:\", hubs)\n",
    "print(\"Authority Scores:\", authorities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dbb7d-1a29-482f-86e7-c533bf6fc8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = np.arange(len(hubs))\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(pages, hubs, color='skyblue')\n",
    "plt.title(\"Hub Scores\")\n",
    "plt.xlabel(\"Page\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(pages, authorities, color='salmon')\n",
    "plt.title(\"Authority Scores\")\n",
    "plt.xlabel(\"Page\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f070a1-cf72-404e-b72c-22466fcb5e1d",
   "metadata": {},
   "source": [
    "## **Implementation with Networkx Module** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda1cf3b-daae-4b93-99e6-58b1ef36c3d5",
   "metadata": {},
   "source": [
    "Let's have **3** iterations $k=3$ (without Normalization)\n",
    "I will display first changes for every iteration ->\n",
    "## 🔢 Initial Hub and Authority Scores\n",
    "\n",
    "| Node | Hub Score | Authority Score |\n",
    "|------|-----------|-----------------|\n",
    "| A    | 1         | 1               |\n",
    "| B    | 1         | 1               |\n",
    "| C    | 1         | 1               |\n",
    "| D    | 1         | 1               |\n",
    "| E    | 1         | 1               |\n",
    "| F    | 1         | 1               |\n",
    "| G    | 1         | 1               |\n",
    "| H    | 1         | 1               |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 After 1st Iteration\n",
    "\n",
    "| Node | Hub Score | Authority Score |\n",
    "|------|-----------|-----------------|\n",
    "| A    | 1         | 3               |\n",
    "| B    | 2         | 2               |\n",
    "| C    | 1         | 4               |\n",
    "| D    | 2         | 2               |\n",
    "| E    | 4         | 1               |\n",
    "| F    | 1         | 1               |\n",
    "| G    | 2         | 0               |\n",
    "| H    | 1         | 1               |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 After 2nd Iteration\n",
    "\n",
    "| Node | Hub Score | Authority Score |\n",
    "|------|-----------|-----------------|\n",
    "| A    | 2         | 4               |\n",
    "| B    | 5         | 6               |\n",
    "| C    | 3         | 7               |\n",
    "| D    | 6         | 5               |\n",
    "| E    | 9         | 2               |\n",
    "| F    | 1         | 4               |\n",
    "| G    | 7         | 0               |\n",
    "| H    | 3         | 1               |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 After 3rd Iteration\n",
    "\n",
    "| Node | Hub Score | Authority Score |\n",
    "|------|-----------|-----------------|\n",
    "| A    | 5         | 13              |\n",
    "| B    | 9         | 15              |\n",
    "| C    | 4         | 27              |\n",
    "| D    | 13        | 11              |\n",
    "| E    | 22        | 5               |\n",
    "| F    | 1         | 9               |\n",
    "| G    | 11        | 0               |\n",
    "| H    | 4         | 3               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b44f80-2161-4f59-b4ad-eb4e821e571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_edges_from([('A', 'D'), ('B', 'C'), ('B', 'E'), ('C', 'A'),\n",
    "                  ('D', 'C'), ('E', 'D'), ('E', 'B'), ('E', 'F'),\n",
    "                  ('E', 'C'), ('F', 'C'), ('F', 'H'), ('G', 'A'), \n",
    "                  ('G', 'C'), ('H', 'A')])\n",
    "\n",
    "plt.figure(figsize =(10, 10))\n",
    "nx.draw_networkx(G, with_labels = True)\n",
    "\n",
    "hubs, authorities = nx.hits(G, max_iter = 50, normalized = True)\n",
    "# The in-built hits function returns two dictionaries keyed by nodes\n",
    "# containing hub scores and authority scores respectively.\n",
    "\n",
    "print(\"Hub Scores: \", hubs)\n",
    "print(\"Authority Scores: \", authorities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c295305-20c3-4136-b040-813dbd48259f",
   "metadata": {},
   "source": [
    "## **BM25 (_Best Matching 25_**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12472c-6db7-4600-8e2e-c63e1082711b",
   "metadata": {},
   "source": [
    "## **Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef638f2-41c0-485d-b6a8-b957e36dafa8",
   "metadata": {},
   "source": [
    "BM25 is a ranking function that scores how relevant a document is to a search query, based on the occurrence of query terms in the document and it improves upon TF-IDF by introducing saturation and document length normalization.\n",
    "When it comes to search engines and information retrieval, a vital piece of the puzzle is ranking the relevance of documents to a given query. One of the most widely used algorithms to achieve this is the BM25, Best Matching 25. BM25 is a probabilistic retrieval function that evaluates the relevance of a document to a search query, balancing simplicity and effectiveness, making it a popular choice in modern search engines and applications.\n",
    "BM25 is essentially a scoring function that calculates a numerical score to estimate the relevance of a document for a given query. This score is based on the occurrences and importance of query terms within the document. The higher the score, the more relevant the document is considered to be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db693fe7-ecc0-4553-b5b5-f6933b850f31",
   "metadata": {},
   "source": [
    "$$  \\sum^{n}_{i= 1}\\text{IDF}(qi).\\frac{f(q_i,D).(k_1 + 1)}{f(q_i,D) + k_1.(1-b + b.\\frac{avgDL}{|D|})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5f540e-84b0-41f9-8870-b8f61bbe0ef6",
   "metadata": {},
   "source": [
    "$( q_i )$ is a term in the query.\n",
    "\n",
    "\n",
    "$f(q_i, D) )$ is the frequency of term $(q_i)$ in document $D$.\n",
    "\n",
    "\n",
    "$|D|$ is the length of document $D$ (number of words).\n",
    "\n",
    "\n",
    "$avgDL$ is the average document length in the corpus.\n",
    "\n",
    "\n",
    "$k_1$ and $b$ are free parameters, where:\n",
    "\n",
    "$k_1$ controls the term frequency saturation, with typical values around 1.2 to 2.\n",
    "\n",
    "$b$ controls the degree of document length normalization, typically set to 0.75.\n",
    "$\\text{IDF}(q_i)$ is the inverse document frequency of term $q_i$.\n",
    "\n",
    "**IDF function**:\n",
    "\n",
    "$$ IDF = \\log \\frac{\\text{Total number of documents in D}}{\\text{Number of documents containing term}}$$\n",
    "\n",
    "The **IDF function** adjusts term weight based on its distribution across documents, giving more importance to rarer terms.\n",
    "\n",
    "### **Key Concepts**\n",
    "BM25 improves upon traditional retrieval models by considering several essential aspects:\n",
    "\n",
    "- Term Frequency (TF): The frequency of a term in the document directly impacts relevance. BM25, however, includes a saturation factor, meaning that additional occurrences of a term add less weight past a certain point, avoiding overemphasis on highly frequent terms.\n",
    "- Inverse Document Frequency (IDF): BM25 uses IDF to balance term frequency by considering how rare or common a term is across documents in the corpus. This way, unique terms in a document are weighted more heavily than common terms.\n",
    "- Document Length Normalization: BM25 incorporates a normalization factor to control the impact of document length on term frequency, which ensures that long documents are not unfairly penalized or favored.\n",
    "- Adjustable Parameters (k1​ and b): BM25 allows flexibility with its two main parameters:\n",
    "k1​ adjusts term frequency scaling, with higher values meaning more emphasis on term frequency.\n",
    "b is a document length normalization parameter that allows BM25 to adapt to different types of datasets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdeb3b3-65b2-4588-9c52-b31fd34fcebd",
   "metadata": {},
   "source": [
    "## **Implementation with Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26e3ca-e5a8-4a5f-813b-0e4377e24847",
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 1.5\n",
    "b = 0.75\n",
    "\n",
    "# Example document collection and query\n",
    "corpus = [\n",
    "    \"the brown fox jumped over the brown dog\",\n",
    "    \"the lazy dog sat in the sun\",\n",
    "    \"the quick brown fox leaped over the lazy dog\"\n",
    "]\n",
    "query = [\"brown\", \"fox\"]\n",
    "\n",
    "# Pre-compute average document length\n",
    "avg_doc_length = sum(len(doc.split()) for doc in corpus) / len(corpus)\n",
    "\n",
    "# Function to calculate term frequency in a document\n",
    "def term_frequency(term, document):\n",
    "    return document.split().count(term)\n",
    "\n",
    "# Function to calculate document frequency for a term\n",
    "def document_frequency(term, corpus):\n",
    "    return sum(1 for doc in corpus if term in doc)\n",
    "\n",
    "# BM25 function\n",
    "def bm25_score(query, document, corpus):\n",
    "    score = 0\n",
    "    doc_length = len(document.split())\n",
    "    for term in query:\n",
    "        tf = term_frequency(term, document)\n",
    "        df = document_frequency(term, corpus)\n",
    "        idf = math.log((len(corpus) - df + 0.5) / (df + 0.5) + 1)\n",
    "        score += idf * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_length / avg_doc_length))))\n",
    "    return score\n",
    "\n",
    "# Calculate BM25 scores for each document\n",
    "for doc in corpus:\n",
    "    print(f\"Document: {doc}\")\n",
    "    print(f\"BM25 Score: {bm25_score(query, doc, corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a05e09d-4313-488e-848a-e80b07e5d3c2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172cdb1f-4a20-4d94-bcfd-8e72aea61b88",
   "metadata": {},
   "source": [
    "This project was made by **Radoslav Todorov** as final project of the course **Math Concepts for Developers** organized by **SoftUni**!\n",
    "\n",
    "## 📚 References\n",
    "- Brin, Sergey, and Lawrence Page. *The Anatomy of a Large-Scale Hypertextual Web Search Engine*, Computer Networks and ISDN Systems, 1998. [link](http://ilpubs.stanford.edu:8090/361/1/1998-8.pdf)\n",
    "- Geeksforgeeks *PageRank*  [link](http://ilpubs.stanford.edu:8090/361/1/1998-8.pdf)\n",
    "- Geeksforgeeks *HITS* [link](https://www.geeksforgeeks.org/hyperlink-induced-topic-search-hits-algorithm-using-networkx-module-python/)\n",
    "- *PageRank*. Wikipedia, [link](https://en.wikipedia.org/wiki/PageRank)\n",
    "- *PageRank*. , [link](https://towardsdatascience.com/pagerank-algorithm-fully-explained-dc794184b4af/)\n",
    "- *HITS*.Wikipedia,[link](https://en.wikipedia.org/wiki/HITS_algorithm)\n",
    "- BM25 [link](https://learncodecamp.net/bm-25/)\n",
    "- Search Engines,Wikipedia [link](https://en.wikipedia.org/wiki/Search_engine)\n",
    "- Search Engines-[link](https://developers.google.com/search/docs/fundamentals/how-search-works)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
